{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\",train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\",train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size = 10, shuffle = True)\n",
    "testset = torch.utils.data.DataLoader(test,batch_size = 10, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object oriented programming model for PyTorch. These libaries are sort of interchangeable, but most code ends up\n",
    "# using both\n",
    "import torch.nn as nn\n",
    "# Functions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "#       Runs initialization for nn.Module as well as everything in the local init.\n",
    "        super().__init__()\n",
    "    \n",
    "        # First fully connected layer. Takes image as input (28 X 28) flattened. \n",
    "        # Output is 3 layers of 64 neurons for the hidden layers\n",
    "        # nn.Linear describes a fully connected neural network     \n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28,64)\n",
    "        # fc2 needs to take in 64 as input because fc1 output 64.\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        # 10 corresponds to the amount of classes we have in image recognition. We only have 10 output neurons\n",
    "        self.fc4 = nn.Linear(64,10)\n",
    "        \n",
    "    # How data will flow through network. F.relu is the activation function. I think sigmoid activation?\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Don't need activation on output of fc4\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.softmax(x,dim=1)\n",
    "        \n",
    "    \n",
    "        \n",
    "net = Net()\n",
    "print(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set X to some random MNIST image \n",
    "X = torch.rand((28,28))\n",
    "# Resize X to pass into net. The -1 specifies that the image has an unknown shape\n",
    "X = X.view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1056, 0.0909, 0.1024, 0.0904, 0.1054, 0.1064, 0.1045, 0.0937, 0.0876,\n",
       "         0.1131]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9748, grad_fn=<NllLossBackward>)\n",
      "tensor(-1., grad_fn=<NllLossBackward>)\n",
      "tensor(-0.9987, grad_fn=<NllLossBackward>)\n",
      "tensor(-1., grad_fn=<NllLossBackward>)\n",
      "tensor(-0.8000, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Training the model to predict handwritten numbers\n",
    "# Loss - How wrong is the model. Goal is to reduce loss\n",
    "# Optimizer - Adjust weights between neurons to lower the loss over time (Based off of learning rate). Calculating\n",
    "# gradients and doing gradient descent\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "# Epochs are how many passes we are doing through our data set.\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Data is a batch of featuresets and labels. It contains the pixel values and the associated labels\n",
    "    for data in trainset:\n",
    "        X,y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        # Calculate how wrong we were\n",
    "        loss = F.nll_loss(output,y)\n",
    "        # Perform backpropagation to see how well the network is doing on a specific example. Calculates the         \n",
    "        # cost for unit j in layer l        \n",
    "        loss.backward()\n",
    "        # Adjust the weights         \n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.952\n",
      "Test accuracy:  0.948\n"
     ]
    }
   ],
   "source": [
    "training_correct = 0\n",
    "training_total = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Checks predictions to see if we were correct     \n",
    "    \n",
    "#     for data in trainset:\n",
    "#         X,y = data\n",
    "#         output = net(X.view(-1,28*28))\n",
    "#         for idx, i in enumerate(output):\n",
    "#             if torch.argmax(i) == y[idx]:\n",
    "#                 training_correct += 1\n",
    "#             training_total += 1\n",
    "        \n",
    "#     for data in testset:\n",
    "#         X,y = data\n",
    "#         output = net(X.view(-1,28*28))\n",
    "#         for idx, i in enumerate(output):\n",
    "#             if torch.argmax(i) == y[idx]:\n",
    "#                 test_correct += 1\n",
    "#             test_total +=1\n",
    "    \n",
    "#     This approach calculates accuracy but avoids the second for loop\n",
    "        for data in trainset:\n",
    "            X,y = data\n",
    "            output = net(X.view(-1,28*28))\n",
    "            training_correct += output.argmax(dim=1).eq(y).sum().item()\n",
    "            training_total += len(y)\n",
    "            \n",
    "        for data in testset:\n",
    "            X,y = data\n",
    "            output = net(X.view(-1,28*28))\n",
    "            test_correct += output.argmax(dim=1).eq(y).sum().item()\n",
    "            test_total += len(y)\n",
    "\n",
    "print(\"Training accuracy: \", round(training_correct/training_total,3))\n",
    "print(\"Test accuracy: \", round(test_correct/test_total,3))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the actual number by passing in the pixel vector\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[2].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# Show the value to see where issues may occur\n",
    "print(torch.argmax(net(X.view(-1,28*28))[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
